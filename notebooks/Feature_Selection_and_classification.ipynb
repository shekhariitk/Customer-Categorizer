{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c53ff9c",
   "metadata": {
    "id": "4c53ff9c"
   },
   "source": [
    "# Classification after Clustering with Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ecb70",
   "metadata": {
    "id": "b45ecb70"
   },
   "source": [
    "### Import CSV and Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a06238",
   "metadata": {
    "id": "93a06238",
    "outputId": "e60c0777-622a-4f97-b4da-c5c1a9d063f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2240, 22)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.pandas.set_option(\"display.max_columns\", None)\n",
    "# Create Dataframe\n",
    "df = pd.read_csv(r\"./data/clustered_data.csv\")\n",
    "# Print shape of dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6ca74",
   "metadata": {
    "id": "e0f6ca74"
   },
   "source": [
    "**Split X and y**\n",
    "- Why do we split our data?\n",
    "> Training Dataset is the part of Original Dataset that we use to train our ML model. The model learns on this data by running the algorithm and maps a function F(x) where “x” in the independent variable (inputs) for “y” where “y” is the dependent variable(output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d3a48e",
   "metadata": {
    "id": "b1d3a48e"
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"cluster\", axis=1) #dropping the target column which is 'cluster'\n",
    "y = df[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398deef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Parental Status</th>\n",
       "      <th>Children</th>\n",
       "      <th>Income</th>\n",
       "      <th>Total_Spending</th>\n",
       "      <th>Days_as_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>Wines</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Meat</th>\n",
       "      <th>Fish</th>\n",
       "      <th>Sweets</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Web</th>\n",
       "      <th>Catalog</th>\n",
       "      <th>Store</th>\n",
       "      <th>Discount Purchases</th>\n",
       "      <th>Total Promo</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>3860</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>81</td>\n",
       "      <td>546</td>\n",
       "      <td>120.5</td>\n",
       "      <td>81</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3014</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>3361</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111.0</td>\n",
       "      <td>21</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2954</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>3210</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46.0</td>\n",
       "      <td>27</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Education  Marital Status  Parental Status  Children   Income  \\\n",
       "0   65          2               0                0         0  58138.0   \n",
       "1   68          2               0                1         2  46344.0   \n",
       "2   57          2               1                0         0  71613.0   \n",
       "3   38          2               1                1         1  26646.0   \n",
       "4   41          4               1                1         1  58293.0   \n",
       "\n",
       "   Total_Spending  Days_as_Customer  Recency  Wines  Fruits  Meat   Fish  \\\n",
       "0          1617.0              3860       58    635      81   546  120.5   \n",
       "1            27.0              3014       38     11       1     6    2.0   \n",
       "2           776.0              3361       26    426      49   127  111.0   \n",
       "3            53.0              2954       26     11       4    20   10.0   \n",
       "4           422.0              3210       94    173      43   118   46.0   \n",
       "\n",
       "   Sweets  Gold  Web  Catalog  Store  Discount Purchases  Total Promo  \\\n",
       "0      81  88.0    8       10      4                   3            0   \n",
       "1       1   6.0    1        1      2                   2            0   \n",
       "2      21  42.0    8        2     10                   1            0   \n",
       "3       3   5.0    2        0      4                   2            0   \n",
       "4      27  15.0    5        3      6                   5            0   \n",
       "\n",
       "   NumWebVisitsMonth  \n",
       "0                  7  \n",
       "1                  5  \n",
       "2                  4  \n",
       "3                  6  \n",
       "4                  5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc221118",
   "metadata": {
    "id": "bc221118"
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "- Why do we use Grid Search?\n",
    "\n",
    "`GridSearchCV` is a technique to search through the best parameter values from the given set of the grid of parameters. It is basically a cross-validation method. the model and the parameters are required to be fed in. Best parameter values are extracted and then the predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e23b9",
   "metadata": {},
   "source": [
    "## Select the best model\n",
    "- so here we have some list of the best classification algorithms we imported. Now we will compare each model's score and see which model is performing better than rest of the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7659f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \\\n",
    "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve,confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn import metrics \n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "     \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"XGBClassifier\": XGBClassifier(), \n",
    "     #\"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477f045",
   "metadata": {},
   "source": [
    "- ### We will create a generic function to check each model's performance so that we can compare those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de9e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which can evaluate models and return a report \n",
    "def evaluate_models(X, y, models):\n",
    "    '''\n",
    "    This function takes in X and y and models dictionary as input\n",
    "    It splits the data into Train Test split\n",
    "    Iterates through the given model dictionary and evaluates the metrics\n",
    "    Returns: Dataframe which contains report of all models metrics with cost\n",
    "    '''\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "\n",
    "    models_list = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = accuracy_score(y_test,y_pred)\n",
    "        \n",
    "        model_name = list(models.keys())[i]\n",
    "        print(f'---- score for --- {model_name} ----')\n",
    "        print(f\"{score}\")\n",
    "        models_list.append(model_name)\n",
    "        scores.append(score)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    report = pd.DataFrame()\n",
    "    report['Model_name'] = models_list\n",
    "    report['Score'] = scores        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371050d9",
   "metadata": {},
   "source": [
    "### Let's check the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6eec8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- score for --- Random Forest ----\n",
      "0.9620535714285714\n",
      "---- score for --- Decision Tree ----\n",
      "0.9419642857142857\n",
      "---- score for --- Gradient Boosting ----\n",
      "0.9598214285714286\n",
      "---- score for --- Logistic Regression ----\n",
      "0.8839285714285714\n",
      "---- score for --- K-Neighbors Classifier ----\n",
      "0.8191964285714286\n",
      "---- score for --- XGBClassifier ----\n",
      "0.9642857142857143\n",
      "---- score for --- AdaBoost Classifier ----\n",
      "0.9017857142857143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = evaluate_models(X, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c80379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_name</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K-Neighbors Classifier</td>\n",
       "      <td>0.819196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.883929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost Classifier</td>\n",
       "      <td>0.901786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.941964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.959821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.962054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model_name     Score\n",
       "4  K-Neighbors Classifier  0.819196\n",
       "3     Logistic Regression  0.883929\n",
       "6     AdaBoost Classifier  0.901786\n",
       "1           Decision Tree  0.941964\n",
       "2       Gradient Boosting  0.959821\n",
       "0           Random Forest  0.962054\n",
       "5           XGBClassifier  0.964286"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.sort_values('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a7528",
   "metadata": {},
   "source": [
    "- ### From the report above we can see that the logistic regression model performed the best, so we will continue training our model using logistic regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73728d0",
   "metadata": {
    "id": "f73728d0"
   },
   "source": [
    "### Split into Train and test data\n",
    "\n",
    "- **Do you know why we split the train and test dataset?**\n",
    "> The train test split technique can be used for classification and regression problems to test machine learning algorithms. The procedure takes the given dataset and splits it into two subsets: ```Training data/train set:``` it is used to train the algorithm and fit the machine learning model\n",
    "then we have ```test data/test set``` which is basically a different data for which we know the values but this data was never shown to the model before. Thus if the model after training is performing good on test set as well then we can say that the Machine Learning model is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130fe7f4",
   "metadata": {
    "id": "130fe7f4",
    "outputId": "4cced242-c7ca-40f1-d1c7-b229cd43ae35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Parental Status</th>\n",
       "      <th>Children</th>\n",
       "      <th>Income</th>\n",
       "      <th>Total_Spending</th>\n",
       "      <th>Days_as_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>Wines</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Meat</th>\n",
       "      <th>Fish</th>\n",
       "      <th>Sweets</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Web</th>\n",
       "      <th>Catalog</th>\n",
       "      <th>Store</th>\n",
       "      <th>Discount Purchases</th>\n",
       "      <th>Total Promo</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64587.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3056</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47320.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>3586</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86429.0</td>\n",
       "      <td>1449.0</td>\n",
       "      <td>3269</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>28</td>\n",
       "      <td>556</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18</td>\n",
       "      <td>37.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38593.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>3338</td>\n",
       "      <td>42</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72905.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>3208</td>\n",
       "      <td>52</td>\n",
       "      <td>407</td>\n",
       "      <td>81</td>\n",
       "      <td>445</td>\n",
       "      <td>120.5</td>\n",
       "      <td>81</td>\n",
       "      <td>126.5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>44078.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3059</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61825.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>3405</td>\n",
       "      <td>56</td>\n",
       "      <td>162</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67381.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>3579</td>\n",
       "      <td>67</td>\n",
       "      <td>815</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48918.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2891</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23228.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3203</td>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1568 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Education  Marital Status  Parental Status  Children   Income  \\\n",
       "994    68          2               0                1         2  64587.0   \n",
       "2162   65          2               1                1         1  47320.0   \n",
       "906    61          2               0                0         0  86429.0   \n",
       "572    44          1               0                1         1  38593.0   \n",
       "1877   64          2               1                0         0  72905.0   \n",
       "...   ...        ...             ...              ...       ...      ...   \n",
       "1638   53          2               1                1         2  44078.0   \n",
       "1095   43          2               0                1         1  61825.0   \n",
       "1130   71          3               1                1         1  67381.0   \n",
       "1294   59          4               0                1         2  48918.0   \n",
       "860    53          2               1                1         1  23228.0   \n",
       "\n",
       "      Total_Spending  Days_as_Customer  Recency  Wines  Fruits  Meat   Fish  \\\n",
       "994            108.0              3056       49     66       0    16    0.0   \n",
       "2162           414.0              3586       10    200      19   111   50.0   \n",
       "906           1449.0              3269       10    464      28   556   29.0   \n",
       "572            177.0              3338       42     51      12    49   17.0   \n",
       "1877          1515.0              3208       52    407      81   445  120.5   \n",
       "...              ...               ...      ...    ...     ...   ...    ...   \n",
       "1638            41.0              3059       17     24       1    10    2.0   \n",
       "1095           424.0              3405       56    162      50   100   55.0   \n",
       "1130           957.0              3579       67    815       8    53   11.0   \n",
       "1294            62.0              2891       21     52       0     9    0.0   \n",
       "860             40.0              3203       71     13       2    18    6.0   \n",
       "\n",
       "      Sweets   Gold  Web  Catalog  Store  Discount Purchases  Total Promo  \\\n",
       "994        6   20.0    1        1      4                   2            0   \n",
       "2162      15   19.0    5        1      8                   6            0   \n",
       "906       18   37.0    7        4      7                   0            1   \n",
       "572       24   24.0    4        1      3                   3            0   \n",
       "1877      81  126.5    3        7      9                   1            1   \n",
       "...      ...    ...  ...      ...    ...                 ...          ...   \n",
       "1638       0    4.0    2        0      3                   2            0   \n",
       "1095      30   27.0    4        2      8                   1            0   \n",
       "1130       0   70.0    2        2      9                   4            1   \n",
       "1294       0    1.0    1        0      4                   2            0   \n",
       "860        1    0.0    2        0      3                   2            0   \n",
       "\n",
       "      NumWebVisitsMonth  \n",
       "994                   3  \n",
       "2162                  6  \n",
       "906                   2  \n",
       "572                   8  \n",
       "1877                  1  \n",
       "...                 ...  \n",
       "1638                  5  \n",
       "1095                  4  \n",
       "1130                  7  \n",
       "1294                  4  \n",
       "860                   8  \n",
       "\n",
       "[1568 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42c967",
   "metadata": {},
   "source": [
    "### Let's do hyperparameter tuning\n",
    "- **And what's it actually?**\n",
    "\n",
    "> A Machine Learning model is defined as a mathematical model with a number of parameters that need to be learned from the data. By training a model with existing data, we are able to fit the model parameters. \n",
    "However, there is another kind of parameter, known as Hyperparameters, that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd70ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
      "tuned hpyerparameters :(best parameters)  {'multi_class': 'auto', 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "accuracy : 0.96875\n"
     ]
    }
   ],
   "source": [
    "# Grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    #'max_iter': np.random.randint(100,500, 10),\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "    #\"C\":np.logspace(-3,3,7),\n",
    "    \"penalty\":[\"l1\",\"l2\"]\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "\n",
    "logreg_cv=GridSearchCV(logreg,params,cv=2,verbose=3, n_jobs=-1, scoring='accuracy')\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb2a8b",
   "metadata": {},
   "source": [
    "### So we got our best parameters. Let's now train the model with those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2937ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_model = LogisticRegression(\n",
    "    C = 1000, \n",
    "    max_iter =  113,\n",
    "    multi_class =  'auto', \n",
    "    penalty =  'l2', \n",
    "    solver =  'newton-cg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b132e02",
   "metadata": {
    "id": "8b132e02"
   },
   "source": [
    "**Initialize model with best parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17843664",
   "metadata": {
    "id": "17843664"
   },
   "source": [
    "### Let's check the report now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8fe3684",
   "metadata": {
    "id": "e8fe3684",
    "outputId": "67e7f682-eaff-4b97-de7c-706cf3af1797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Accuracy Score value: 0.9702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       179\n",
      "           1       0.94      0.98      0.96       235\n",
      "           2       0.99      0.96      0.97       258\n",
      "\n",
      "    accuracy                           0.97       672\n",
      "   macro avg       0.97      0.97      0.97       672\n",
      "weighted avg       0.97      0.97      0.97       672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = best_lr_model.fit(X_train,y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "cr = classification_report(y_test,y_pred)\n",
    "\n",
    "print(\"Logistic regression\")\n",
    "print (\"Accuracy Score value: {:.4f}\".format(score))\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf81d62",
   "metadata": {},
   "source": [
    "## Confusion matrix of the model\n",
    "- **What is confusion matrix ?**\n",
    "> The confusion matrix is a matrix used to determine the performance of the classification models for a given set of test data. It can only be determined if the true values for test data are known. The matrix itself can be easily understood, but the related terminologies may be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78ddb30e",
   "metadata": {
    "id": "78ddb30e",
    "outputId": "4e62cc7f-7bed-4aa1-85aa-8df275d3750c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1f7e5e43ee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGwCAYAAACuFMx9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAANQxJREFUeJzt3Xl4VOX5//HPJCEbZCFAEgJhE9kEwYLGuCBoSoAWpfD9uhRtBMSvmKCCqFDLXk3rhkURWhci/YFgVbBQi0V2ZGlBcQOiQBQQwiKSkGC2mfP7I2V0TNQMZyaTOef9uq5z1Tnb3CnKnft+nvMch2EYhgAAgGWFBDoAAADgXyR7AAAsjmQPAIDFkewBALA4kj0AABZHsgcAwOJI9gAAWFxYoAMww+Vy6ciRI4qJiZHD4Qh0OAAALxmGoTNnziglJUUhIf6rP8vKylRRUWH6PuHh4YqMjPRBRPUrqJP9kSNHlJqaGugwAAAmHTp0SK1bt/bLvcvKytS+bRMVHneavldycrIKCgqCLuEHdbKPiYmRJF396iiFRYcHOBr43a++CnQEqEdGVWWgQ0A9qDIqtVn/cP997g8VFRUqPO7UFzvbKTbm/LsHxWdcatv7c1VUVJDs69O51n1YdLjCGkcEOBr4naNRoCNAPTIYmbMPQ/UyFNskxqEmMef/PS4F77+UQZ3sAQCoK6fhktPE22Cchst3wdQzkj0AwBZcMuTS+Wd7M9cGGo/eAQBgcVT2AABbcMklM414c1cHFskeAGALTsOQ0zj/VryZawONNj4AABZHZQ8AsAU7T9Aj2QMAbMElQ06bJnva+AAAWByVPQDAFmjjAwBgcczGBwAAlkVlDwCwBdd/NzPXByuSPQDAFpwmZ+ObuTbQSPYAAFtwGjL51jvfxVLfGLMHAMDiqOwBALbAmD0AABbnkkNOOUxdH6xo4wMAYHFU9gAAW3AZ1ZuZ64MVyR4AYAtOk218M9cGGm18AAAsjsoeAGALdq7sSfYAAFtwGQ65DBOz8U1cG2i08QEAsDgqewCALdDGBwDA4pwKkdNEQ9vpw1jqG8keAGALhskxe4MxewAA0FBR2QMAbIExewAALM5phMhpmBizD+LlcmnjAwBgcVT2AABbcMkhl4ka16XgLe1J9gAAW7DzmD1tfAAALI7KHgBgC+Yn6NHGBwCgQaseszfxIhza+AAAoKGisgcA2ILL5Nr4zMYHAKCBY8weAACLcynEts/ZM2YPAIDFUdkDAGzBaTjkNPGaWjPXBhrJHgBgC06TE/SctPEBAEBDRWUPALAFlxEil4nZ+C5m4wMA0LDRxgcAAJZFZQ8AsAWXzM2od/kulHpHsgcA2IL5RXWCtxkevJEDAIA6obIHANiC+bXxg7c+JtkDAGzBzu+zJ9kDAGyByh4BZ3xQLi0tkT6rkL5ySTMT5Lgq6tvj135Z+4V3xspxc4znvSoMKfuEtL9S+ksLOTqG+zN0+NmNY49q1KTDWvZikv48s02gw4GPdU8r0f+OPa4Le5xVs+QqTR/VTlvfjg90WLCYBvFryty5c9WuXTtFRkYqLS1N//73vwMdUv0rM6QLGkn3xNd+/LVkz+2BeMkhqW9UzXP/UiQ1axB/tDCp08UlGjziuA7sruXPGZYQGe3Sgd1Revbh1oEOxfLOLapjZgtWAY986dKlmjBhgqZNm6b33ntPPXv2VGZmpo4fPx7o0OqVIy1SjtGxclxd+1/qjoRQj01byqReEXKkeDZnjO1l0o5y6a64+ggbfhQZ7dSDfzqgPz3UTiVFNOGsase6WL38WEttWRUf6FAsz2U4TG/BKuDJ/qmnntKYMWM0cuRIdevWTfPnz1d0dLReeumlQIfWYBmnnNK2MmlwdM39T34tTW4qRQbvv5Solj3rC/17bbzef5df3ACYE9BkX1FRoZ07dyojI8O9LyQkRBkZGdq6dWuN88vLy1VcXOyx2dK/zkrRDuk7XQDDMKTHvpaGNJajM2P0we6aIV+pY/ezWvAYrV3AV1wmW/gsqnOeTp48KafTqaSkJI/9SUlJKiwsrHF+bm6u4uLi3Ftqamp9hdqw/POsdF20HOHfqd6XlUpnDenXMT98HYJC85blumvaQT12bwdVlgfvXy5AQ3PurXdmtmAVVAOBkydP1oQJE9yfi4uLbZfwjQ/LpUNV0tQEzwPvl0u7K6TMI57vZbrrhIyMKDkmfe98NFgX9jirpi2q9Ow/PnHvCw2Tuqed0fVZxzTkwj5yuRimAVB3AU32zZs3V2hoqI4dO+ax/9ixY0pOTq5xfkREhCIiIuorvIbpn2elTo3kuKCR5/6cOGlU7LefTzqlh76q/qWgK239YLLr3Vj9388v8th3/xMFOrQ/Sq/OSybRA+fJKYecJhbGMXNtoAU02YeHh6t3795as2aNhg4dKklyuVxas2aNcnJyAhlavTO+cUlfVn2746hTxr4KKSZEjqTqPyaj1CVt+KbWmfbnznHfL+q//1KmhMrRItRvccP3vikN1Refek6+LDsbquKvw2rsR/CLjHYqpX25+3Nymwp1uOisznwdphNH+EXdl8y24mnjmzBhwgRlZWWpT58+uuyyy/T000+rtLRUI0eODHRo9Su/Uppw8tvP84qq/zczWnqoafU/r/tGMiRdyzPXgFV06nlWj7+23/35rulHJEn/erWpnhzfNlBhwWICnuxvuukmnThxQlOnTlVhYaF69eqlVatW1Zi0Z3WOXhHS2lY/fs4vG0u/bFy3+yWH/eT9EDwevLlLoEOAn3y4NUaZrXoFOgxbcMpcK97p5fm5ubl64403tHfvXkVFRemKK67QH//4R3Xu3Nl9TllZme6//34tWbJE5eXlyszM1HPPPeeRAw8ePKixY8dq3bp1atKkibKyspSbm6uwsLqn8AbRk8jJydEXX3yh8vJybd++XWlpaYEOCQBgMfU9G3/Dhg3Kzs7Wtm3btHr1alVWVmrAgAEqLS11nzN+/HitWLFCf/vb37RhwwYdOXJEw4YNcx93Op36xS9+oYqKCm3ZskUvv/yy8vLyNHXqVK9iCXhlDwBAffDVi3C+v8bLD00eX7VqlcfnvLw8JSYmaufOnerbt6+Kior04osvavHixbr22mslSQsWLFDXrl21bds2XX755frXv/6l3bt365133lFSUpJ69eqlWbNm6aGHHtL06dMVHl63eR0NorIHACBYpKameqz5kpubW6frioqq52IlJFQ/Cr1z505VVlZ6LCzXpUsXtWnTxr2w3NatW9WjRw+Ptn5mZqaKi4v1ySefqK6o7AEAtmCYfJ+98d9rDx06pNjYbx91rssj4S6XS/fdd5+uvPJKde/eXZJUWFio8PBwxcfHe5z73YXlCgsLa1147tyxuiLZAwBswVdt/NjYWI9kXxfZ2dn6+OOPtXnz5vP+fjNo4wMA4Ec5OTlauXKl1q1bp9atv33fRXJysioqKnT69GmP87+7sFxycnKtC8+dO1ZXJHsAgC3U9ytuDcNQTk6Oli1bprVr16p9+/Yex3v37q1GjRppzZo17n35+fk6ePCg0tPTJUnp6en66KOPPF77vnr1asXGxqpbt251joU2PgDAFs69vc7M9d7Izs7W4sWL9eabbyomJsY9xh4XF6eoqCjFxcVp9OjRmjBhghISEhQbG6tx48YpPT1dl19+uSRpwIAB6tatm2677TY99thjKiws1O9+9ztlZ2d7tXw8yR4AAD+YN2+eJKlfv34e+xcsWKDbb79dkjR79myFhIRo+PDhHovqnBMaGqqVK1dq7NixSk9PV+PGjZWVlaWZM2d6FQvJHgBgC+fTiv/+9d4wDOMnz4mMjNTcuXM1d+7cHzynbdu2euutt7z67u8j2QMAbMGlELlMtPHNXBtowRs5AACoEyp7AIAtOA2HnCba+GauDTSSPQDAFup7zL4hIdkDAGzBOI83133/+mAVvJEDAIA6obIHANiCUw45TbwIx8y1gUayBwDYgsswN+7u+unH5hss2vgAAFgclT0AwBZcJifombk20Ej2AABbcMkhl4lxdzPXBlrw/poCAADqhMoeAGALrKAHAIDF2XnMPngjBwAAdUJlDwCwBZdMro0fxBP0SPYAAFswTM7GN0j2AAA0bHZ+6x1j9gAAWByVPQDAFuw8G59kDwCwBdr4AADAsqjsAQC2YOe18Un2AABboI0PAAAsi8oeAGALdq7sSfYAAFuwc7KnjQ8AgMVR2QMAbMHOlT3JHgBgC4bMPT5n+C6UekeyBwDYgp0re8bsAQCwOCp7AIAt2LmyJ9kDAGzBzsmeNj4AABZHZQ8AsAU7V/YkewCALRiGQ4aJhG3m2kCjjQ8AgMVR2QMAbIH32QMAYHF2HrOnjQ8AgMVR2QMAbMHOE/RI9gAAW7BzG59kDwCwBTtX9ozZAwBgcdao7IeekByNAh0F/Gz2vo2BDgH16L72VwY6BFiMYbKNH8yVvTWSPQAAP8GQZBjmrg9WtPEBALA4KnsAgC245JCDFfQAALAuZuMDAADLorIHANiCy3DIwaI6AABYl2GYnI0fxNPxaeMDAGBxVPYAAFuw8wQ9kj0AwBZI9gAAWJydJ+gxZg8AgMVR2QMAbMHOs/FJ9gAAW6hO9mbG7H0YTD2jjQ8AgMVR2QMAbIHZ+AAAWJwhc++kD+IuPm18AACsjmQPALCFc218M5s3Nm7cqCFDhiglJUUOh0PLly/3OH777bfL4XB4bAMHDvQ459SpUxoxYoRiY2MVHx+v0aNHq6SkxOufnWQPALAHwwebF0pLS9WzZ0/NnTv3B88ZOHCgjh496t5eeeUVj+MjRozQJ598otWrV2vlypXauHGj7rzzTu8CEWP2AAC7MDlBT/+9tri42GN3RESEIiIiapw+aNAgDRo06EdvGRERoeTk5FqP7dmzR6tWrdJ//vMf9enTR5L0zDPPaPDgwXriiSeUkpJS59Cp7AEA8EJqaqri4uLcW25u7nnfa/369UpMTFTnzp01duxYffXVV+5jW7duVXx8vDvRS1JGRoZCQkK0fft2r76Hyh4AYAu+WkHv0KFDio2Nde+vraqvi4EDB2rYsGFq37699u/fr9/+9rcaNGiQtm7dqtDQUBUWFioxMdHjmrCwMCUkJKiwsNCr7yLZAwBswVfP2cfGxnok+/N18803u/+5R48euvjii3XBBRdo/fr1uu6660zf/7to4wMA0AB06NBBzZs31759+yRJycnJOn78uMc5VVVVOnXq1A+O8/8Qkj0AwB4Mh/nNjw4fPqyvvvpKLVu2lCSlp6fr9OnT2rlzp/uctWvXyuVyKS0tzat708YHANhCfb/1rqSkxF2lS1JBQYF27dqlhIQEJSQkaMaMGRo+fLiSk5O1f/9+Pfjgg+rYsaMyMzMlSV27dtXAgQM1ZswYzZ8/X5WVlcrJydHNN9/s1Ux8icoeAAC/2LFjhy655BJdcsklkqQJEybokksu0dSpUxUaGqoPP/xQ119/vTp16qTRo0erd+/e2rRpk8eEv0WLFqlLly667rrrNHjwYF111VX6y1/+4nUsVPYAAHuo58Xx+/XrJ+NH2gFvv/32T94jISFBixcv9u6La0GyBwDYAm+9+wl///vf63zD66+//ryDAQAAvlenZD906NA63czhcMjpdJqJBwAA/wnm99SaUKdk73K5/B0HAAB+Zec2vqnZ+GVlZb6KAwAA/6rnt941JF4ne6fTqVmzZqlVq1Zq0qSJDhw4IEmaMmWKXnzxRZ8HCAAAzPE62T/yyCPKy8vTY489pvDwcPf+7t2764UXXvBpcAAA+I7DB1tw8jrZL1y4UH/5y180YsQIhYaGuvf37NlTe/fu9WlwAAD4DG38uvvyyy/VsWPHGvtdLpcqKyt9EhQAAPAdr5N9t27dtGnTphr7X3vtNfeSgAAANDg2ruy9XkFv6tSpysrK0pdffimXy6U33nhD+fn5WrhwoVauXOmPGAEAMM/sm+vs9OjdDTfcoBUrVuidd95R48aNNXXqVO3Zs0crVqzQz3/+c3/ECAAATDivtfGvvvpqrV692texAADgN/X9ituG5LxfhLNjxw7t2bNHUvU4fu/evX0WFAAAPlfPb71rSLxO9ocPH9Ytt9yid999V/Hx8ZKk06dP64orrtCSJUvUunVrX8cIAABM8HrM/o477lBlZaX27NmjU6dO6dSpU9qzZ49cLpfuuOMOf8QIAIB55ybomdmClNeV/YYNG7RlyxZ17tzZva9z58565plndPXVV/s0OAAAfMVhVG9mrg9WXif71NTUWhfPcTqdSklJ8UlQAAD4nI3H7L1u4z/++OMaN26cduzY4d63Y8cO3XvvvXriiSd8GhwAADCvTpV906ZN5XB8O1ZRWlqqtLQ0hYVVX15VVaWwsDCNGjVKQ4cO9UugAACYYuNFdeqU7J9++mk/hwEAgJ/ZuI1fp2SflZXl7zgAAICfnPeiOpJUVlamiooKj32xsbGmAgIAwC9sXNl7PUGvtLRUOTk5SkxMVOPGjdW0aVOPDQCABsnGb73zOtk/+OCDWrt2rebNm6eIiAi98MILmjFjhlJSUrRw4UJ/xAgAAEzwuo2/YsUKLVy4UP369dPIkSN19dVXq2PHjmrbtq0WLVqkESNG+CNOAADMsfFsfK8r+1OnTqlDhw6SqsfnT506JUm66qqrtHHjRt9GBwCAj5xbQc/MFqy8ruw7dOiggoICtWnTRl26dNGrr76qyy67TCtWrHC/GAe+94tbT+iXt51QYutySdLBT6O06E8ttWN9XIAjg7dWz22lD99upuP7o9Qo0qV2PyvWkElfKOmCMvc5Syd30Kfvxqv4WCOFN3ap/c/OVJ/T8Rv3Oa9Pb6+CHTE6+mm0ki74Rg/+84NA/DgwqXtaif537HFd2OOsmiVXafqodtr6dnygw4LFeF3Zjxw5Uh98UP2XyqRJkzR37lxFRkZq/PjxeuCBB7y618aNGzVkyBClpKTI4XBo+fLl3oZjGycLG+mlP7TSuF901T2/7KpdW2I07YX9atvpm5++GA3K/u2xuuq2o7pv2Yca+9dP5KoK0fzfXKTys9/+55jao1S/fnyfJr2zS3ct3C1D0rzfdJPL6XmvtBuP65JfnqzfHwA+FRnt0oHdUXr2Yd4Y6nc2nqDndWU/fvx49z9nZGRo79692rlzpzp27KiLL77Yq3uVlpaqZ8+eGjVqlIYNG+ZtKLay/Z14j88vP95Kv7zthLpcUqovPo0KTFA4L3ct3OPx+ddPfKbf9b5Mhz9qogvSiiVJV/z6mPt4s9Ry/eL+g3psUC+dOhyh5m2ruzvDpxdIkkq+StWRPY3rKXr42o51sdqxjkeW4V+mnrOXpLZt26pt27bnde2gQYM0aNAgsyHYTkiIoat/8bUiolza8x5/yQe7b85U/2cYHV9V6/HysyHa/rdENUstU3zLilrPAfDTHDL51jufRVL/6pTs58yZU+cb3nPPPecdzE8pLy9XeXm5+3NxcbHfvqshatf5G81evlfhES59UxqqWXdeoIOfUdUHM5dLWjazndr3KVbLzmc9jm3+a7L+nttWFWdDldjhrMb+v08UFh7EfUQAAVOnZD979uw63czhcPg12efm5mrGjBl+u39Dd/hAhO4e2FWNY526evBp3f/U53rwxk4k/CD22pQOOpofrXtf+7jGsd43nFDnq06r+Hi41j6forzszrr3tY/UKJKED5wXGz96V6dkX1BQ4O846mTy5MmaMGGC+3NxcbFSU1MDGFH9qqoM0dEvIiVJ+z5qrE49SzV01HHNmXx+wygIrNemttfutU017tWPa23PR8U6FRXrVIv2ZWp7yRn9tudl+vDtZup9AxPygPNi4+VyTY/Z16eIiAhFREQEOowGw+GQGtHWDTqGIb0+rb0+ejtBOUs+UbPU8jpcVH1dVUXwVhYAAieokr2djXzoS/1nXaxOHAlXVGOX+g89pYvTz+jh2y4MdGjw0mtTOmjnm811x/N7FdHYqeLjjSRJkbFOhUe6dPJghN5f0Vxd+p5Wk4RKnS6M0DvzWqlRpEvd+p923+fE55EqLw3RmRONVFkeosOfREuSki/8hrH9IBIZ7VRK+29/4UtuU6EOF53Vma/DdOJIeAAjsyAq+8AoKSnRvn373J8LCgq0a9cuJSQkqE2bNgGMrOGJb1apB2Z/rqaJlTp7JlQFe6P08G0X6v1NPLITbN79f8mSpGdv7u6x/5bHP1Pa/55QowiXDvwnVhsWtNQ3RWGKaV6pCy4r1r2vf6SY5pXu85c8dIH2b/92UaUnftFLkjRl0866dQvQIHTqeVaPv7bf/fmu6UckSf96tameHM8QnS+ZXQXPVivo+dKOHTvUv39/9+dz4/FZWVnKy8sLUFQN0+wH2wU6BPjI059v+dHjcUmV+r+8PT96jiSNW/qJr0JCAH24NUaZrXoFOgxYXECTfb9+/WQYQfyrEgAgeNi4je/1crmStGnTJt16661KT0/Xl19+KUn661//qs2bN/s0OAAAfMbGy+V6nexff/11ZWZmKioqSu+//757kZuioiI9+uijPg8QAACY43Wy//3vf6/58+fr+eefV6NGjdz7r7zySr333ns+DQ4AAF/hFbdeyM/PV9++fWvsj4uL0+nTp30REwAAvmfjFfS8ruyTk5M9Hpc7Z/PmzerQoYNPggIAwOcYs6+7MWPG6N5779X27dvlcDh05MgRLVq0SBMnTtTYsWP9ESMAADDB6zb+pEmT5HK5dN111+ns2bPq27evIiIiNHHiRI0bN84fMQIAYBqL6njB4XDo4Ycf1gMPPKB9+/appKRE3bp1U5MmTfwRHwAAvmHj5+zPe1Gd8PBwdevWzZexAAAAP/A62ffv318Oxw/PSFy7dq2pgAAA8Auzj8/ZqbLv1auXx+fKykrt2rVLH3/8sbKysnwVFwAAvkUbv+5mz55d6/7p06erpKTEdEAAAMC3zmtt/Nrceuuteumll3x1OwAAfMvGz9n77K13W7duVWRkpK9uBwCAT/HonReGDRvm8dkwDB09elQ7duzQlClTfBYYAADwDa+TfVxcnMfnkJAQde7cWTNnztSAAQN8FhgAAPANr5K90+nUyJEj1aNHDzVt2tRfMQEA4Hs2no3v1QS90NBQDRgwgLfbAQCCjp1fcev1bPzu3bvrwIED/ogFAAD4gdfJ/ve//70mTpyolStX6ujRoyouLvbYAABosGz42J3kxZj9zJkzdf/992vw4MGSpOuvv95j2VzDMORwOOR0On0fJQAAZtl4zL7OyX7GjBm66667tG7dOn/GAwAAfKzOyd4wqn+lueaaa/wWDAAA/sKiOnX0Y2+7AwCgQbNxG9+rCXqdOnVSQkLCj24AAEDauHGjhgwZopSUFDkcDi1fvtzjuGEYmjp1qlq2bKmoqChlZGTos88+8zjn1KlTGjFihGJjYxUfH6/Ro0ef10vnvKrsZ8yYUWMFPQAAgkF9t/FLS0vVs2dPjRo1qsZS85L02GOPac6cOXr55ZfVvn17TZkyRZmZmdq9e7f7XTMjRozQ0aNHtXr1alVWVmrkyJG68847tXjxYq9i8SrZ33zzzUpMTPTqCwAAaBDquY0/aNAgDRo0qPZbGYaefvpp/e53v9MNN9wgSVq4cKGSkpK0fPly3XzzzdqzZ49WrVql//znP+rTp48k6ZlnntHgwYP1xBNPKCUlpc6x1LmNz3g9AACqsb5MeXm51/coKChQYWGhMjIy3Pvi4uKUlpamrVu3Sqp+m2x8fLw70UtSRkaGQkJCtH37dq++r87J/txsfAAAgpKP3mefmpqquLg495abm+t1KIWFhZKkpKQkj/1JSUnuY4WFhTW66WFhYUpISHCfU1d1buO7XC6vbgwAQEPiqzH7Q4cOKTY21r0/IiLCZGT+5/VyuQAABCUfVfaxsbEe2/kk++TkZEnSsWPHPPYfO3bMfSw5OVnHjx/3OF5VVaVTp065z6krkj0AAPWsffv2Sk5O1po1a9z7iouLtX37dqWnp0uS0tPTdfr0ae3cudN9ztq1a+VyuZSWlubV93k1Gx8AgKBVz7PxS0pKtG/fPvfngoIC7dq1SwkJCWrTpo3uu+8+/f73v9eFF17ofvQuJSVFQ4cOlSR17dpVAwcO1JgxYzR//nxVVlYqJydHN998s1cz8SWSPQDAJur7OfsdO3aof//+7s8TJkyQJGVlZSkvL08PPvigSktLdeedd+r06dO66qqrtGrVKvcz9pK0aNEi5eTk6LrrrlNISIiGDx+uOXPmeB07yR4AAD/o16/fjz7J5nA4NHPmTM2cOfMHz0lISPB6AZ3akOwBAPZg47XxSfYAAFuw81vvmI0PAIDFUdkDAOyBNj4AABZn42RPGx8AAIujsgcA2ILjv5uZ64MVyR4AYA82buOT7AEAtsCjdwAAwLKo7AEA9kAbHwAAGwjihG0GbXwAACyOyh4AYAt2nqBHsgcA2IONx+xp4wMAYHFU9gAAW6CNDwCA1dHGBwAAVmWJyt6oqpLhCOZXFKAu7u81KNAhoB69/eWGQIeAelB8xqWmnernu2jjAwBgdTZu45PsAQD2YONkz5g9AAAWR2UPALAFxuwBALA62vgAAMCqqOwBALbgMAw5jPMvz81cG2gkewCAPdDGBwAAVkVlDwCwBWbjAwBgdbTxAQCAVVHZAwBsgTY+AABWZ+M2PskeAGALdq7sGbMHAMDiqOwBAPZAGx8AAOsL5la8GbTxAQCwOCp7AIA9GEb1Zub6IEWyBwDYArPxAQCAZVHZAwDsgdn4AABYm8NVvZm5PljRxgcAwOKo7AEA9kAbHwAAa7PzbHySPQDAHmz8nD1j9gAAWByVPQDAFmjjAwBgdTaeoEcbHwAAi6OyBwDYAm18AACsjtn4AADAqqjsAQC2QBsfAACrYzY+AACwKip7AIAt0MYHAMDqXEb1Zub6IEWyBwDYA2P2AADAqqjsAQC24JDJMXufRVL/SPYAAHtgBT0AAOBL06dPl8Ph8Ni6dOniPl5WVqbs7Gw1a9ZMTZo00fDhw3Xs2DG/xEKyBwDYwrlH78xs3rrooot09OhR97Z582b3sfHjx2vFihX629/+pg0bNujIkSMaNmyYD3/ib9HGBwDYQwBm44eFhSk5ObnG/qKiIr344otavHixrr32WknSggUL1LVrV23btk2XX365iUBrorIHAMALxcXFHlt5efkPnvvZZ58pJSVFHTp00IgRI3Tw4EFJ0s6dO1VZWamMjAz3uV26dFGbNm20detWn8dMsgcA2ILDMExvkpSamqq4uDj3lpubW+v3paWlKS8vT6tWrdK8efNUUFCgq6++WmfOnFFhYaHCw8MVHx/vcU1SUpIKCwt9/rPTxgcA2IPrv5uZ6yUdOnRIsbGx7t0RERG1nj5o0CD3P1988cVKS0tT27Zt9eqrryoqKspEIN6jsgcAwAuxsbEe2w8l+++Lj49Xp06dtG/fPiUnJ6uiokKnT5/2OOfYsWO1jvGbRbIHANiCr9r456ukpET79+9Xy5Yt1bt3bzVq1Ehr1qxxH8/Pz9fBgweVnp5u9ketgTY+AMAe6nk2/sSJEzVkyBC1bdtWR44c0bRp0xQaGqpbbrlFcXFxGj16tCZMmKCEhATFxsZq3LhxSk9P9/lMfIlkDwCwi3peQe/w4cO65ZZb9NVXX6lFixa66qqrtG3bNrVo0UKSNHv2bIWEhGj48OEqLy9XZmamnnvuufOP70eQ7AEA8IMlS5b86PHIyEjNnTtXc+fO9XssJHsAgC2c7yp4370+WJHsg8yQ20/qf8YeV0KLKh3YHaXnftdK+buiAx0WTOre+7SGjzqsjheVqFlihWaN66ata5q7j1+RcVKDbzqijheVKDa+SjnDfqYDe5sEMGLUxZJnEvXuW/E6tC9C4ZEudetzVqMfPqLUjjUXYTEM6Xe3dtCOdbGa9mKBrhhUJEn619IEPTm+Ta33X/rhx4pvXuXXn8FSeBEOgsE113+tO6cd0aKnkpWd2UkHdkfqkcUHFNesMtChwaTIaJcK8hvruVkdaz8e5dQn78VpwZPt6zkymPHh1iYacvtJPb3yM+Uu2S9nlfTbWy5Q2dmaf/Uue76FHLW8Q/Wa67/WK7s+9th69yvWxeklJHrUWUCTfW5uri699FLFxMQoMTFRQ4cOVX5+fiBDatCG3XlSqxYn6F9LE3Tws0jNeai1yr9xKPOWU4EODSbt2JSghXPae1Tz37V2RZJemddW729tWs+RwYxHFx/QgJtOqV3nMl1wUZnuf/qgjn8Zrs8+9FxQZf/HUXr9zy004amDNe4REWUoIbHKvYWEGvrg3SbKvOWr+voxLMPhMr8Fq4Am+w0bNig7O1vbtm3T6tWrVVlZqQEDBqi0tDSQYTVIYY1cuvDis3pvU4x7n2E49P6mGHXrfTaAkQGoq9LiUElSTLzTva/srEN/yG6r7EcOKyHxpyv1d/6WoIgoQ1f/4rS/wrSuc218M1uQCuiY/apVqzw+5+XlKTExUTt37lTfvn1rnF9eXu7xwoHi4mK/x9hQxCY4FRomnT7h+Uf29cmwWsf/ADQsLpc0f1orXXRpidp1KXPv//P0VurWp1RXDKzb32dvv9JM/X/1tSKigjfxoP41qDH7oqLqCSkJCQm1Hs/NzfV4+UBqamp9hgcA5+3Z37bWF3ujNHneF+59W9+O1a53Y3TXzC/rdI/dO6J18LNIDaSFf34MH2xBqsEke5fLpfvuu09XXnmlunfvXus5kydPVlFRkXs7dOhQPUcZOMWnQuWskuJbeLb5mjav0tcneKgCaMie/W0rbV8dq8de26cWKd9OqN31boyOfh6uYV16aFBqTw1K7SlJmjWmnR4YXnOy5qrFzXTBRWd14cXf1FvsVhLo5XIDqcFkiezsbH388cfavHnzD54TERFR5xcOWE1VZYg++zBal1x1RltXxUmSHA5Dva4q0d/zmgU4OgC1MQxp7sOttGVVnB5/bZ+S21R4HL8p55gG/dqzSv+/a7vo/6Z/qcsHeLb1vykN0cYV8Ro5+ajf44b1NIhkn5OTo5UrV2rjxo1q3bp1oMNpsN74S3NNfPqQPv0gWvnvR+tXY04oMtqlfy2pfdgDwSMy2qmUNt9Wa0mtytShS4nOFIXpxNFINYmrVGLLciUkVieL1u2qJ2V+fTJcX58MD0jM+GnP/ra11i1rqukLDiiqiUunjlf/lds4xukxy/77EltV1vjFYMOb8XI6Hbpu+Nf1Ersl2fg5+4Ame8MwNG7cOC1btkzr169X+/Y8Q/xjNvy9qeKaOfWbBwrVtEWVDnwSpYdHtNfpk40CHRpMuvCiM/rjyx+6P9856YAkafWyJM1+uLMu7/+VJjz6qfv4pKf2SpIWzW2jRXPb1WusqLuVL1c/SvnA8As99t8/+6AG3OTdI7OrXmmmKwedVpM450+fjNoZMvc+++DN9XIYRuB+Vbn77ru1ePFivfnmm+rcubN7f1xcnKKion7kymrFxcWKi4tTP92gMAcJz+pC4+MCHQLq0Vu7NwQ6BNSD4jMuNe10QEVFRYqNjfXPd/w3V1x7ySSFhUae932qnGVa+/4f/BqrvwR0gt68efNUVFSkfv36qWXLlu5t6dKlgQwLAABLCXgbHwCAemHI5Ji9zyKpdw1igh4AAH5n4wl6DeY5ewAA4B9U9gAAe3BJquXNgl5dH6RI9gAAWzC7Cl4wr6BHGx8AAIujsgcA2IONJ+iR7AEA9mDjZE8bHwAAi6OyBwDYg40re5I9AMAeePQOAABr49E7AABgWVT2AAB7YMweAACLcxmSw0TCdgVvsqeNDwCAxVHZAwDsgTY+AABWZzLZK3iTPW18AAAsjsoeAGAPtPEBALA4lyFTrXhm4wMAgIaKyh4AYA+Gq3ozc32QItkDAOyBMXsAACyOMXsAAGBVVPYAAHugjQ8AgMUZMpnsfRZJvaONDwCAxVHZAwDsgTY+AAAW53JJMvGsvCt4n7OnjQ8AgMVR2QMA7IE2PgAAFmfjZE8bHwAAi6OyBwDYg42XyyXZAwBswTBcMky8uc7MtYFGsgcA2INhmKvOGbMHAAANFZU9AMAeDJNj9kFc2ZPsAQD24HJJDhPj7kE8Zk8bHwAAi6OyBwDYA218AACszXC5ZJho4wfzo3e08QEAsDgqewCAPdDGBwDA4lyG5LBnsqeNDwCAxVHZAwDswTAkmXnOPngre5I9AMAWDJchw0Qb3yDZAwDQwBkumavsefQOAADUYu7cuWrXrp0iIyOVlpamf//73/UeA8keAGALhsswvXlr6dKlmjBhgqZNm6b33ntPPXv2VGZmpo4fP+6Hn/CHkewBAPZguMxvXnrqqac0ZswYjRw5Ut26ddP8+fMVHR2tl156yQ8/4A8L6jH7c5MlqlRpap0EBAfDqAh0CKhHxWeCd3wUdVdcUv3nXB+T38zmiipVSpKKi4s99kdERCgiIqLG+RUVFdq5c6cmT57s3hcSEqKMjAxt3br1/AM5D0Gd7M+cOSNJ2qy3AhwJ6sXpQAeA+tS0U6AjQH06c+aM4uLi/HLv8PBwJScna3Oh+VzRpEkTpaameuybNm2apk+fXuPckydPyul0KikpyWN/UlKS9u7dazoWbwR1sk9JSdGhQ4cUExMjh8MR6HDqTXFxsVJTU3Xo0CHFxsYGOhz4EX/W9mHXP2vDMHTmzBmlpKT47TsiIyNVUFCgigrz3UHDMGrkm9qq+oYmqJN9SEiIWrduHegwAiY2NtZWfynYGX/W9mHHP2t/VfTfFRkZqcjISL9/z3c1b95coaGhOnbsmMf+Y8eOKTk5uV5jYYIeAAB+EB4ert69e2vNmjXufS6XS2vWrFF6enq9xhLUlT0AAA3ZhAkTlJWVpT59+uiyyy7T008/rdLSUo0cObJe4yDZB6GIiAhNmzYtKMaJYA5/1vbBn7U13XTTTTpx4oSmTp2qwsJC9erVS6tWraoxac/fHEYwL/YLAAB+EmP2AABYHMkeAACLI9kDAGBxJHsAACyOZB9kGsKrEuF/Gzdu1JAhQ5SSkiKHw6Hly5cHOiT4SW5uri699FLFxMQoMTFRQ4cOVX5+fqDDgsWQ7INIQ3lVIvyvtLRUPXv21Ny5cwMdCvxsw4YNys7O1rZt27R69WpVVlZqwIABKi0tDXRosBAevQsiaWlpuvTSS/Xss89Kql6JKTU1VePGjdOkSZMCHB38xeFwaNmyZRo6dGigQ0E9OHHihBITE7Vhwwb17ds30OHAIqjsg8S5VyVmZGS49wXqVYkA/KeoqEiSlJCQEOBIYCUk+yDxY69KLCwsDFBUAHzJ5XLpvvvu05VXXqnu3bsHOhxYCMvlAkADkZ2drY8//libN28OdCiwGJJ9kGhIr0oE4Hs5OTlauXKlNm7caOtXd8M/aOMHiYb0qkQAvmMYhnJycrRs2TKtXbtW7du3D3RIsCAq+yDSUF6VCP8rKSnRvn373J8LCgq0a9cuJSQkqE2bNgGMDL6WnZ2txYsX680331RMTIx7Dk5cXJyioqICHB2sgkfvgsyzzz6rxx9/3P2qxDlz5igtLS3QYcHH1q9fr/79+9fYn5WVpby8vPoPCH7jcDhq3b9gwQLdfvvt9RsMLItkDwCAxTFmDwCAxZHsAQCwOJI9AAAWR7IHAMDiSPYAAFgcyR4AAIsj2QMAYHEkewAALI5kD5h0++23a+jQoe7P/fr103333Vfvcaxfv14Oh0OnT5/+wXMcDoeWL19e53tOnz5dvXr1MhXX559/LofDoV27dpm6D4DzR7KHJd1+++1yOBxyOBwKDw9Xx44dNXPmTFVVVfn9u9944w3NmjWrTufWJUEDgFm8CAeWNXDgQC1YsEDl5eV66623lJ2drUaNGmny5Mk1zq2oqFB4eLhPvjchIcEn9wEAX6Gyh2VFREQoOTlZbdu21dixY5WRkaG///3vkr5tvT/yyCNKSUlR586dJUmHDh3SjTfeqPj4eCUkJOiGG27Q559/7r6n0+nUhAkTFB8fr2bNmunBBx/U918v8f02fnl5uR566CGlpqYqIiJCHTt21IsvvqjPP//c/bKbpk2byuFwuF984nK5lJubq/bt2ysqKko9e/bUa6+95vE9b731ljp16qSoqCj179/fI866euihh9SpUydFR0erQ4cOmjJliiorK2uc9+c//1mpqamKjo7WjTfeqKKiIo/jL7zwgrp27arIyEh16dJFzz33nNexAPAfkj1sIyoqShUVFe7Pa9asUX5+vlavXq2VK1eqsrJSmZmZiomJ0aZNm/Tuu++qSZMmGjhwoPu6J598Unl5eXrppZe0efNmnTp1SsuWLfvR7/3Nb36jV155RXPmzNGePXv05z//WU2aNFFqaqpef/11SVJ+fr6OHj2qP/3pT5Kk3NxcLVy4UPPnz9cnn3yi8ePH69Zbb9WGDRskVf9SMmzYMA0ZMkS7du3SHXfcoUmTJnn9/0lMTIzy8vK0e/du/elPf9Lzzz+v2bNne5yzb98+vfrqq1qxYoVWrVql999/X3fffbf7+KJFizR16lQ98sgj2rNnjx599FFNmTJFL7/8stfxAPATA7CgrKws44YbbjAMwzBcLpexevVqIyIiwpg4caL7eFJSklFeXu6+5q9//avRuXNnw+VyufeVl5cbUVFRxttvv20YhmG0bNnSeOyxx9zHKysrjdatW7u/yzAM45prrjHuvfdewzAMIz8/35BkrF69utY4161bZ0gyvv76a/e+srIyIzo62tiyZYvHuaNHjzZuueUWwzAMY/LkyUa3bt08jj/00EM17vV9koxly5b94PHHH3/c6N27t/vztGnTjNDQUOPw4cPuff/85z+NkJAQ4+jRo4ZhGMYFF1xgLF682OM+s2bNMtLT0w3DMIyCggJDkvH+++//4PcC8C/G7GFZK1euVJMmTVRZWSmXy6Vf//rXmj59uvt4jx49PMbpP/jgA+3bt08xMTEe9ykrK9P+/ftVVFSko0ePKi0tzX0sLCxMffr0qdHKP2fXrl0KDQ3VNddcU+e49+3bp7Nnz+rnP/+5x/6KigpdcsklkqQ9e/Z4xCFJ6enpdf6Oc5YuXao5c+Zo//79KikpUVVVlWJjYz3OadOmjVq1auXxPS6XS/n5+YqJidH+/fs1evRojRkzxn1OVVWV4uLivI4HgH+Q7GFZ/fv317x58xQeHq6UlBSFhXn+6964cWOPzyUlJerdu7cWLVpU414tWrQ4rxiioqK8vqakpESS9I9//MMjyUrV8xB8ZevWrRoxYoRmzJihzMxMxcXFacmSJXryySe9jvX555+v8ctHaGioz2IFYA7JHpbVuHFjdezYsc7n/+xnP9PSpUuVmJhYo7o9p2XLltq+fbv69u0rqbqC3blzp372s5/Ven6PHj3kcrm0YcMGZWRk1Dh+rrPgdDrd+7p166aIiAgdPHjwBzsCXbt2dU82PGfbtm0//UN+x5YtW9S2bVs9/PDD7n1ffPFFjfMOHjyoI0eOKCUlxf09ISEh6ty5s5KSkpSSkqIDBw5oxIgRXn0/gPrDBD3gv0aMGKHmzZvrhhtu0KZNm1RQUKD169frnnvu0eHDhyVJ9957r/7whz9o+fLl2rt3r+6+++4ffUa+Xbt2ysrK0qhRo7R8+XL3PV999VVJUtu2beVwOLRy5UqdOHFCJSUliomJ0cSJEzV+/Hi9/PLL2r9/v9577z0988wz7klvd911lz777DM98MADys/P1+LFi5WXl+fVz3vhhRfq4MGDWrJkifbv3685c+bUOtkwMjJSWVlZ+uCDD7Rp0ybdc889uvHGG5WcnCxJmjFjhnJzczVnzhx9+umn+uijj7RgwQI99dRTXsUDwH9I9sB/RUdHa+PGjWrTpo2GDRumrl27avTo0SorK3NX+vfff79uu+02ZWVlKT09XTExMfrVr371o/edN2+e/ud//kd33323unTpojFjxqi0tFSS1KpVK82YMUOTJk1SUlKScnJyJEmzZs3SlClTlJubq65du2rgwIH6xz/+ofbt20uqHkd//fXXtXz5cvXs2VPz58/Xo48+6tXPe/3112v8+PHKyclRr169tGXLFk2ZMqXGeR07dtSwYcM0ePBgDRgwQBdffLHHo3V33HGHXnjhBS1YsEA9evTQNddco7y8PHesAALPYfzQzCIAAGAJVPYAAFgcyR4AAIsj2QMAYHEkewAALI5kDwCAxZHsAQCwOJI9AAAWR7IHAMDiSPYAAFgcyR4AAIsj2QMAYHH/Hw+AitCPSe6YAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401046a4",
   "metadata": {},
   "source": [
    "- **Reports**\n",
    "\n",
    "**We can see, that the model performed pretty well.**\n",
    "- we have used logistic regression as it performed well than other models\n",
    "- We got a good accuracy while predicting the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde78a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
